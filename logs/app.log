2022-07-19 19:49:39,934 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 19:49:39,976 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-19 19:51:34,161 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 19:51:34,185 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-19 19:53:16,165 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 19:53:16,180 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-19 20:08:57,934 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 20:08:57,972 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-19 20:11:01,690 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 20:11:01,718 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-19 20:19:06,387 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-19 20:19:06,426 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-21 18:51:32,177 WARN o.a.h.u.Shell [main] Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
2022-07-21 18:51:32,193 WARN o.a.h.u.NativeCodeLoader [main] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-07-21 21:22:42,349 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-23 16:04:31,003 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-23 16:07:15,677 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-23 16:55:26,134 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:07:35,189 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:15:48,898 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:15:50,065 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-a286f564-a57d-4083-84cc-3c0aa7c38e31. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:15:50,102 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:16:45,400 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:16:50,504 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-195cde53-09f5-4489-9375-525ec8d120de. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:16:50,519 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:16:50,979 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 90d547b8-0c85-43b6-b879-f509b0844b18, runId = ee708beb-c2d7-4351-8710-765fdb2d4187]] Query customer_purchases [id = 90d547b8-0c85-43b6-b879-f509b0844b18, runId = ee708beb-c2d7-4351-8710-765fdb2d4187] terminated with error
java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2220) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:32:21,541 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:32:25,061 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-02f3289a-6895-4ba9-bb36-b7fea27d1ef4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:32:25,077 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:40:56,741 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:40:58,995 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-b0d2e345-8faa-4aa1-8fec-7f8ff2ff78b0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:40:59,024 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:41:53,570 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:41:58,117 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-73867186-b137-4180-8c41-0167a5c84c9d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:41:58,149 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:41:58,587 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 45eee5fb-bcbf-4bb3-af9d-f34b38b99f84, runId = a07bd181-3563-4ab4-9321-b3e0ad37c3cb]] Query customer_purchases [id = 45eee5fb-bcbf-4bb3-af9d-f34b38b99f84, runId = a07bd181-3563-4ab4-9321-b3e0ad37c3cb] terminated with error
java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2220) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:42:28,233 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:42:32,174 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-37e36f56-7bd7-4e7d-bd40-2ede88d611d9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:42:32,191 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:42:32,616 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 28b1d20c-bdbb-4dfa-ad2e-7adaf21ef17a, runId = 7bde4f9b-a590-403b-8f54-338075631418]] Query customer_purchases [id = 28b1d20c-bdbb-4dfa-ad2e-7adaf21ef17a, runId = 7bde4f9b-a590-403b-8f54-338075631418] terminated with error
java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2220) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:44:01,933 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:44:05,342 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-38b9b0e6-227f-4068-ab49-472989f3e8ac. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:44:05,374 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:44:39,494 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-ab9a3a3a-1000-4c3f-b5bd-4e6553ff9fe3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:44:39,521 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:45:21,996 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:45:27,362 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-b9871932-20d4-4d05-8bd8-dca680ef1dba. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:45:27,388 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:45:27,923 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 117220b0-9df4-477c-8dd2-47783bc90d30, runId = edb1fdcf-3f6a-409b-9e5e-44e0e695553c]] Query customer_purchases [id = 117220b0-9df4-477c-8dd2-47783bc90d30, runId = edb1fdcf-3f6a-409b-9e5e-44e0e695553c] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:333) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:46:31,453 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:46:31,642 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-9dbac3d3-76b6-4ee2-b749-deabe1d51629. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:46:31,657 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:46:32,096 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 5acf4837-dafa-48bb-ba03-357c3083d48c, runId = ea92b90e-3041-4e90-b8b9-290f49f23123]] Query customer_purchases [id = 5acf4837-dafa-48bb-ba03-357c3083d48c, runId = ea92b90e-3041-4e90-b8b9-290f49f23123] terminated with error
java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2220) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:47:37,270 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:47:41,607 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-fac738a9-beaf-4b64-a583-3d971d635425. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:47:41,638 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:47:42,085 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = de46ef4c-b7e9-4e0c-a563-6bdebf628eb0, runId = 944f2a0f-4226-47fa-8a80-1f7c13b9ab79]] Query customer_purchases [id = de46ef4c-b7e9-4e0c-a563-6bdebf628eb0, runId = 944f2a0f-4226-47fa-8a80-1f7c13b9ab79] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:333) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:48:13,411 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:48:16,111 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-c5163d0c-1dad-4b35-893d-7242e7489103. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:48:16,142 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:48:47,279 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:48:52,510 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-f9a05549-a286-4151-a5de-89784fb03342. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:48:52,541 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:48:52,976 ERROR o.a.s.s.e.s.MicroBatchExecution [stream execution thread for customer_purchases [id = 46e6cafd-0d37-4a2c-b96a-fdc1542177fe, runId = 1dcaa54a-11a0-4f70-a544-2f5cda0b367d]] Query customer_purchases [id = 46e6cafd-0d37-4a2c-b96a-fdc1542177fe, runId = 1dcaa54a-11a0-4f70-a544-2f5cda0b367d] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:333) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:265) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:317) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:139) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:340) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:448) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:447) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:246) ~[scala-library-2.13.8.jar:?]
	at scala.collection.immutable.List.map(List.scala:79) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:436) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:687) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:432) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:237) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)

The currently active SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
com.github.ristinak.Day19StreamingExample$.delayedEndpoint$com$github$ristinak$Day19StreamingExample$1(Day19StreamingExample.scala:11)
com.github.ristinak.Day19StreamingExample$delayedInit$body.apply(Day19StreamingExample.scala:9)
scala.Function0.apply$mcV$sp(Function0.scala:39)
scala.Function0.apply$mcV$sp$(Function0.scala:39)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
scala.App.$anonfun$main$1(App.scala:76)
scala.App.$anonfun$main$1$adapted(App.scala:76)
scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
scala.collection.AbstractIterable.foreach(Iterable.scala:926)
scala.App.main(App.scala:76)
scala.App.main$(App.scala:74)
com.github.ristinak.Day19StreamingExample$.main(Day19StreamingExample.scala:9)
com.github.ristinak.Day19StreamingExample.main(Day19StreamingExample.scala)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-07-24 18:49:30,815 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-9a2ba3ec-cd70-44b5-8c26-466a58579593. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:49:30,846 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:51:18,695 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-7d8b6cfc-274d-41ea-9b9c-37861a1bf434. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:51:18,739 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:54:04,800 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:54:05,182 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-e6c7fb64-b608-435e-8b71-1b79c2084bca. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:54:05,206 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 18:54:58,689 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 18:54:59,581 WARN o.a.s.s.e.s.ResolveWriteToStream [main] Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\Users\risti\AppData\Local\Temp\temporary-c12a2c9a-5e07-4284-9d11-e812b73933fd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2022-07-24 18:54:59,613 WARN o.a.s.s.e.s.ResolveWriteToStream [main] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2022-07-24 19:01:14,437 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 19:13:08,571 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-20] Error in removing shuffle 0
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) ~[?:?]
2022-07-24 19:13:08,603 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-18] Failed to remove shuffle 0 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) ~[?:?]
2022-07-24 19:13:09,920 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-24 19:15:14,486 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-07-25 21:04:58,499 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-01 09:28:21,779 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-01 09:28:21,933 WARN o.a.s.s.e.w.WindowExec [main] No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
2022-08-01 09:28:21,937 WARN o.a.s.s.e.w.WindowExec [main] No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
2022-08-01 09:28:21,937 WARN o.a.s.s.e.w.WindowExec [main] No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
2022-08-09 18:36:58,568 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 18:36:59,405 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 18:52:05,059 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 18:52:56,392 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 18:52:56,586 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 19:05:47,336 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 19:05:49,362 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 20:32:18,942 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 20:32:19,320 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:35:04,175 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:36:00,339 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:36:44,579 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:37:42,312 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:38:20,234 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:41:23,278 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:42:11,211 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:42:13,409 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 20:45:24,801 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:46:07,455 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:49:01,232 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:49:51,883 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:50:37,146 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:51:20,443 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:52:17,402 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 20:54:50,518 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 21:14:38,600 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 21:14:38,728 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-09 21:15:33,081 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 21:16:06,496 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 21:19:13,655 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-09 21:22:38,051 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-11 10:18:00,305 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-11 10:18:01,889 WARN o.a.s.s.e.CacheManager [main] Asked to cache already cached data.
2022-08-11 10:18:02,696 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-11 20:43:02,240 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-11 20:46:19,449 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-11 20:49:20,053 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-11 21:19:48,687 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-12 15:36:22,983 WARN o.a.s.s.c.u.package [main] Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2022-08-12 15:36:24,645 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 09:52:00,836 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:19:08,640 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:23:17,783 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:40:14,915 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:42:26,117 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:46:19,203 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:52:18,415 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-13 10:57:22,001 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-15 18:13:48,014 WARN o.a.p.CorruptStatistics [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)
org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) using format: (.*?)\s+version\s*(?:([^(]*?)\s*(?:\(\s*build\s*([^)]*?)\s*\))?)?
	at org.apache.parquet.VersionParser.parse(VersionParser.java:109) ~[parquet-common-1.12.2.jar:1.12.2]
	at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:72) ~[parquet-column-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatisticsInternal(ParquetMetadataConverter.java:811) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:831) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.buildColumnChunkMetaData(ParquetMetadataConverter.java:1462) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:1535) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:1450) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:582) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:99) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:173) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:343) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-15 18:13:48,125 WARN o.a.h.i.c.z.ZlibFactory [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Failed to load/initialize native-zlib library
2022-08-15 18:14:31,640 ERROR o.a.s.u.Utils [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] Aborting task
java.io.FileNotFoundException: 
File file:/C:/Users/risti/IdeaProjects/Spark_Kristina_2022/src/resources/flight-data/parquet/2010-summary_fixed.parquet/part-00000-0ec98ba2-4eff-462a-ab93-cb76a33c8557-c000.snappy.parquet does not exist

It is possible the underlying files have been updated. You can explicitly invalidate
the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
recreating the Dataset/DataFrame involved.
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-15 18:14:31,664 ERROR o.a.s.s.e.d.FileFormatWriter [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] Job job_202208151814315592572845478859119_0005 aborted.
2022-08-15 18:14:31,664 ERROR o.a.s.e.Executor [Executor task launch worker for task 0.0 in stage 5.0 (TID 4)] Exception in task 0.0 in stage 5.0 (TID 4)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
Caused by: java.io.FileNotFoundException: 
File file:/C:/Users/risti/IdeaProjects/Spark_Kristina_2022/src/resources/flight-data/parquet/2010-summary_fixed.parquet/part-00000-0ec98ba2-4eff-462a-ab93-cb76a33c8557-c000.snappy.parquet does not exist

It is possible the underlying files have been updated. You can explicitly invalidate
the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
recreating the Dataset/DataFrame involved.
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	... 9 more
2022-08-15 18:14:31,683 WARN o.a.s.s.TaskSetManager [task-result-getter-0] Lost task 0.0 in stage 5.0 (TID 4) (LAPTOP-89ESGVHE executor driver): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: 
File file:/C:/Users/risti/IdeaProjects/Spark_Kristina_2022/src/resources/flight-data/parquet/2010-summary_fixed.parquet/part-00000-0ec98ba2-4eff-462a-ab93-cb76a33c8557-c000.snappy.parquet does not exist

It is possible the underlying files have been updated. You can explicitly invalidate
the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
recreating the Dataset/DataFrame involved.
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)
	... 9 more

2022-08-15 18:14:31,683 ERROR o.a.s.s.TaskSetManager [task-result-getter-0] Task 0 in stage 5.0 failed 1 times; aborting job
2022-08-15 18:14:31,690 ERROR o.a.s.s.e.d.FileFormatWriter [main] Aborting job 7a9f02f9-de6d-42c4-b809-d2b566dd3d48.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 4) (LAPTOP-89ESGVHE executor driver): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: 
File file:/C:/Users/risti/IdeaProjects/Spark_Kristina_2022/src/resources/flight-data/parquet/2010-summary_fixed.parquet/part-00000-0ec98ba2-4eff-462a-ab93-cb76a33c8557-c000.snappy.parquet does not exist

It is possible the underlying files have been updated. You can explicitly invalidate
the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
recreating the Dataset/DataFrame involved.
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)
	... 9 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:333) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at com.github.ristinak.Day31Parquet$.delayedEndpoint$com$github$ristinak$Day31Parquet$1(Day31Parquet.scala:43) ~[classes/:?]
	at com.github.ristinak.Day31Parquet$delayedInit$body.apply(Day31Parquet.scala:5) ~[classes/:?]
	at scala.Function0.apply$mcV$sp(Function0.scala:39) ~[scala-library-2.13.8.jar:?]
	at scala.Function0.apply$mcV$sp$(Function0.scala:39) ~[scala-library-2.13.8.jar:?]
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17) ~[scala-library-2.13.8.jar:?]
	at scala.App.$anonfun$main$1(App.scala:76) ~[scala-library-2.13.8.jar:?]
	at scala.App.$anonfun$main$1$adapted(App.scala:76) ~[scala-library-2.13.8.jar:?]
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563) ~[scala-library-2.13.8.jar:?]
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561) ~[scala-library-2.13.8.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926) ~[scala-library-2.13.8.jar:?]
	at scala.App.main(App.scala:76) ~[scala-library-2.13.8.jar:?]
	at scala.App.main$(App.scala:74) ~[scala-library-2.13.8.jar:?]
	at com.github.ristinak.Day31Parquet$.main(Day31Parquet.scala:5) ~[classes/:?]
	at com.github.ristinak.Day31Parquet.main(Day31Parquet.scala) ~[classes/:?]
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
Caused by: java.io.FileNotFoundException: 
File file:/C:/Users/risti/IdeaProjects/Spark_Kristina_2022/src/resources/flight-data/parquet/2010-summary_fixed.parquet/part-00000-0ec98ba2-4eff-462a-ab93-cb76a33c8557-c000.snappy.parquet does not exist

It is possible the underlying files have been updated. You can explicitly invalidate
the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
recreating the Dataset/DataFrame involved.
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:648) ~[spark-catalyst_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-15 18:15:55,024 WARN o.a.p.CorruptStatistics [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)
org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) using format: (.*?)\s+version\s*(?:([^(]*?)\s*(?:\(\s*build\s*([^)]*?)\s*\))?)?
	at org.apache.parquet.VersionParser.parse(VersionParser.java:109) ~[parquet-common-1.12.2.jar:1.12.2]
	at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:72) ~[parquet-column-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatisticsInternal(ParquetMetadataConverter.java:811) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:831) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.buildColumnChunkMetaData(ParquetMetadataConverter.java:1462) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:1535) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:1450) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:582) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776) ~[parquet-hadoop-1.12.2.jar:1.12.2]
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:99) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:173) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:343) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364) ~[spark-sql_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-15 18:15:55,116 WARN o.a.h.i.c.z.ZlibFactory [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Failed to load/initialize native-zlib library
2022-08-15 22:02:42,342 WARN o.a.s.s.e.d.j.JdbcUtils [Executor task launch worker for task 0.0 in stage 8.0 (TID 6)] Requested isolation level 1 is not supported; falling back to default isolation level 8
2022-08-16 20:40:52,523 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-18 15:18:37,731 WARN o.a.s.e.ProcfsMetricsGetter [executor-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-18 15:18:43,215 ERROR o.a.s.ContextCleaner [Spark Context Cleaner] Error cleaning shuffle 16
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-08-18 15:18:43,232 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-53] Error in removing shuffle 17
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:43,234 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-49] Failed to remove shuffle 17 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:44,732 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-11] Error in removing shuffle 22
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:44,733 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-45] Failed to remove shuffle 22 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:44,754 ERROR o.a.s.ContextCleaner [Spark Context Cleaner] Error cleaning shuffle 20
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-08-18 15:18:47,770 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-37] Error in removing shuffle 34
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:47,770 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-83] Error in removing shuffle 35
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:47,771 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-89] Failed to remove shuffle 34 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:47,771 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-88] Failed to remove shuffle 35 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:49,665 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-58] Error in removing shuffle 37
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:49,666 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-76] Failed to remove shuffle 37 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:49,685 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-59] Error in removing shuffle 36
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:18:49,686 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-37] Failed to remove shuffle 36 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:19:42,108 WARN o.a.s.e.ProcfsMetricsGetter [driver-heartbeater] Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
2022-08-18 15:19:50,439 ERROR o.a.s.ContextCleaner [Spark Context Cleaner] Error cleaning shuffle 15
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-08-18 15:19:51,615 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-42] Error in removing shuffle 17
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:19:51,617 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-4] Failed to remove shuffle 17 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:19:51,617 ERROR o.a.s.ContextCleaner [Spark Context Cleaner] Error cleaning shuffle 19
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) ~[spark-core_2.13-3.3.0.jar:3.3.0]
2022-08-18 15:19:53,781 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-18] Error in removing shuffle 21
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:19:53,782 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-47] Failed to remove shuffle 21 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:20:01,650 ERROR o.a.s.s.BlockManagerStorageEndpoint [block-manager-storage-async-thread-pool-39] Error in removing shuffle 49
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:20:01,651 WARN o.a.s.s.BlockManagerMaster [block-manager-ask-thread-pool-83] Failed to remove shuffle 49 - null
java.lang.NullPointerException: null
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.Option.foreach(Option.scala:437) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17) ~[scala-library-2.13.8.jar:?]
	at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89) ~[spark-core_2.13-3.3.0.jar:3.3.0]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678) ~[scala-library-2.13.8.jar:?]
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467) ~[scala-library-2.13.8.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_342]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_342]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]
2022-08-18 15:41:04,352 WARN c.g.r.Day18Logging [main] This is a warning
2022-08-18 15:41:04,359 ERROR c.g.r.Day18Logging [main] This is an error!
